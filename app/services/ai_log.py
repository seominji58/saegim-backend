"""
AI í…ìŠ¤íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ (OpenAI API ì‚¬ìš©)
"""

import json
import logging
import time
import uuid
from datetime import UTC, datetime
from enum import Enum
from typing import Any
from uuid import UUID

from sqlalchemy import func, select
from sqlalchemy.orm import Session

from app.exceptions.ai import (
    AIGenerationFailedException,
    InvalidRequestException,
    RegenerationLimitExceededException,
    SessionNotFoundException,
)
from app.models.ai_usage_log import AIUsageLog
from app.schemas.create_diary import CreateDiaryRequest
from app.services.base import BaseService
from app.services.notification_service import NotificationService
from app.utils.openai_utils import get_openai_client

logger = logging.getLogger(__name__)


class EmotionType(str, Enum):
    """ê°ì • íƒ€ì… (diary_ai.pyì—ì„œ ê°€ì ¸ì˜´)"""

    HAPPINESS = "í–‰ë³µ"
    SADNESS = "ìŠ¬í””"
    ANGER = "í™”ë‚¨"
    PEACE = "í‰ì˜¨"
    UNREST = "ë¶ˆì•ˆ"


class WritingStyle(str, Enum):
    """ê¸€ ë¬¸ì²´ íƒ€ì…"""

    POEM = "poem"  # ì‹œ
    SHORT_STORY = "short_story"  # ë‹¨í¸ê¸€


class ContentLength(str, Enum):
    """ê¸€ê·€ ê¸¸ì´ íƒ€ì…"""

    SHORT = "short"  # ë‹¨ë¬¸ (1-2ë¬¸ì¥)
    MEDIUM = "medium"  # ì¤‘ë¬¸ (3-5ë¬¸ì¥)
    LONG = "long"  # ì¥ë¬¸ (6-10ë¬¸ì¥)


class AIService(BaseService):
    def __init__(self, db: Session):
        super().__init__(db)
        if not self.db:
            raise ValueError("Database session is required for AIService")
        # íƒ€ì… ì²´ì»¤ë¥¼ ìœ„í•œ ëª…ì‹œì  ì–´ì„œì…˜
        assert isinstance(self.db, Session), "AIService requires a Session instance"

    @property
    def session(self) -> Session:
        """íƒ€ì… ì•ˆì „í•œ ì„¸ì…˜ ì ‘ê·¼"""
        if not self.db or not isinstance(self.db, Session):
            raise ValueError("Database session is required for AIService")
        return self.db

    async def generate_ai_text(
        self,
        user_id: UUID,
        data: CreateDiaryRequest,
    ) -> dict[str, Any]:
        try:
            logger.info(f"AI í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: {data.prompt[:50]}...")

            start_time = time.time()
            # í†µí•©ëœ AI ë¶„ì„ ë° ê¸€ê·€ ìƒì„± (í•œ ë²ˆì˜ API í˜¸ì¶œ)
            ai_result = await self._generate_complete_analysis(
                data.prompt, data.style, data.length
            )
            logger.info("í†µí•© AI ë¶„ì„ ë° ê¸€ê·€ ìƒì„± ì™„ë£Œ")

            # ê²°ê³¼ íŒŒì‹±
            emotion_analysis = {
                "emotion": ai_result["emotion"],
                "confidence": ai_result.get("confidence", 0.9),
                "details": f"í†µí•© ë¶„ì„: {ai_result['emotion']}",
            }
            keywords = ai_result["keywords"]
            ai_response = {
                "text": ai_result["generated_text"],
                "tokens_used": ai_result["tokens_used"],
            }

            # ì¬ìƒì„± ì‹œ session_id ë˜ëŠ” sessionIdë¥¼ ê°™ì´ ì „ë‹¬ ë°›ìŒ
            # openai ìš”ì²­ ìƒì„±(ìµœì´ˆ ìš”ì²­ì€ session_id ìƒì„± ë° regeneration_count 1)
            session_id = (
                getattr(data, "sessionId", None) or data.session_id or str(uuid.uuid4())
            )

            # ê¸°ì¡´ ì„¸ì…˜ ë¡œê·¸ ì¡°íšŒ (í†µí•© ë¶„ì„ íƒ€ì…ìœ¼ë¡œ)
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
            )

            existing_logs = self.session.execute(statement).scalars().all()
            regeneration_count = len(existing_logs) + 1

            # ì¬ìƒì„± íšŸìˆ˜ ì œí•œ ì²´í¬ (5íšŒê¹Œì§€ë§Œ í—ˆìš©)
            if regeneration_count > 5:
                logger.warning(
                    f"âš ï¸ ì¬ìƒì„± íšŸìˆ˜ ì´ˆê³¼: ì‚¬ìš©ì {user_id}, ì„¸ì…˜ {session_id}, ì‹œë„ íšŸìˆ˜: {regeneration_count}"
                )
                raise RegenerationLimitExceededException(
                    current_count=regeneration_count, max_count=5, session_id=session_id
                )

            # í†µí•© AI ë¶„ì„ ë¡œê·¸ ì €ì¥ (ê°ì •ë¶„ì„ + í‚¤ì›Œë“œì¶”ì¶œ + ê¸€ê·€ìƒì„±)
            ai_usage_log = AIUsageLog(
                user_id=user_id,
                api_type="integrated_analysis",  # ìƒˆë¡œìš´ í†µí•© ë¶„ì„ íƒ€ì…
                session_id=session_id,
                request_data=data.model_dump(),
                response_data={
                    "ai_generated_text": ai_response["text"],
                    "emotion": emotion_analysis["emotion"],
                    "emotion_confidence": emotion_analysis["confidence"],
                    "keywords": keywords,
                    "style": data.style,
                    "length": data.length,
                },
                tokens_used=ai_response.get("tokens_used", 0),
                regeneration_count=regeneration_count,
            )
            self.session.add(ai_usage_log)

            # ë°ì´í„°ë² ì´ìŠ¤ ì»¤ë°‹
            self.session.commit()
            logger.info(
                f"í†µí•© AI ë¶„ì„ ë¡œê·¸ ì €ì¥ ì™„ë£Œ - ID: {ai_usage_log.id}, í† í° ì‚¬ìš©ëŸ‰: {ai_response.get('tokens_used', 0)}"
            )

            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° ë° ì¡°ê±´ë¶€ ì•Œë¦¼ ë°œì†¡
            processing_time = time.time() - start_time
            NOTIFICATION_THRESHOLD_SECONDS = 3.0  # 3ì´ˆ ì´ìƒ ê±¸ë¦° ê²½ìš°ì—ë§Œ ì•Œë¦¼ ë°œì†¡

            if processing_time >= NOTIFICATION_THRESHOLD_SECONDS:
                logger.info(
                    f"AI í…ìŠ¤íŠ¸ ìƒì„± ì‹œê°„ì´ {processing_time:.2f}ì´ˆë¡œ ì„ê³„ê°’({NOTIFICATION_THRESHOLD_SECONDS}ì´ˆ) ì´ˆê³¼, ì•Œë¦¼ ë°œì†¡"
                )
                try:
                    notification_service = NotificationService(self.session)
                    notification_result = (
                        await notification_service.send_ai_content_ready(
                            user_id, session_id
                        )
                    )
                    if notification_result.success_count > 0:
                        logger.info(
                            f"AI í…ìŠ¤íŠ¸ ìƒì„± í›„ ì½˜í…ì¸  ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì„±ê³µ: session_id={session_id}, user_id={user_id}, ì²˜ë¦¬ì‹œê°„={processing_time:.2f}ì´ˆ"
                        )
                    else:
                        logger.warning(
                            f"AI í…ìŠ¤íŠ¸ ìƒì„± í›„ ì½˜í…ì¸  ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: session_id={session_id}, user_id={user_id}"
                        )
                except Exception as e:
                    # ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨ê°€ AI í…ìŠ¤íŠ¸ ìƒì„±ì„ ë°©í•´í•˜ì§€ ì•Šë„ë¡ í•¨
                    logger.error(
                        f"AI í…ìŠ¤íŠ¸ ìƒì„± í›„ ì½˜í…ì¸  ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: session_id={session_id}, user_id={user_id}, error={str(e)}"
                    )
            else:
                logger.info(
                    f"AI í…ìŠ¤íŠ¸ ìƒì„± ì‹œê°„ì´ {processing_time:.2f}ì´ˆë¡œ ë¹ ë¦„, ì•Œë¦¼ ë°œì†¡ ìƒëµ"
                )

            # ì‹¤ì œ í”„ë¡ íŠ¸ì— í•„ìš”í•œ ì‘ë‹µ ìƒì„±
            result = {
                "ai_generated_text": ai_response["text"],
                "ai_emotion": emotion_analysis.get(
                    "emotion", data.emotion or "neutral"
                ),
                "ai_emotion_confidence": emotion_analysis.get("confidence", 0.85),
                "keywords": keywords,
                "session_id": session_id,
                "regeneration_info": {
                    "current_count": regeneration_count,
                    "max_count": 5,
                    "remaining_count": 5 - regeneration_count,
                    "can_regenerate": regeneration_count < 5,
                },
            }
            # ì‘ë‹µ
            return result
        except RegenerationLimitExceededException:
            # ì¬ìƒì„± ì œí•œ ì˜ˆì™¸ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
            raise
        except Exception as e:
            logger.error(f"AI í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise AIGenerationFailedException(
                detail=f"AI í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                error_type="GENERATION_ERROR",
            ) from e

    async def stream_ai_text(
        self,
        user_id: UUID,
        data: CreateDiaryRequest,
    ):
        """AI í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìƒì„±"""
        try:
            logger.info(f"AI í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {data.prompt[:50]}...")

            # ì„¸ì…˜ ID ìƒì„±/í™•ì¸
            session_id = (
                getattr(data, "sessionId", None) or data.session_id or str(uuid.uuid4())
            )

            # ì¬ìƒì„± íšŸìˆ˜ í™•ì¸
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
            )
            existing_logs = self.session.execute(statement).scalars().all()
            regeneration_count = len(existing_logs) + 1

            if regeneration_count > 5:
                error_data = {
                    "error": "ì¬ìƒì„± íšŸìˆ˜ê°€ 5íšŒë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
                    "session_id": session_id,
                    "current_count": regeneration_count,
                }
                yield json.dumps(error_data, ensure_ascii=False)
                return

            # ì´ˆê¸° ë©”íƒ€ë°ì´í„° ì „ì†¡
            initial_data = {
                "type": "start",
                "session_id": session_id,
                "regeneration_count": regeneration_count,
            }
            yield json.dumps(initial_data, ensure_ascii=False)

            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
            collected_text = ""
            total_tokens = 0
            chunk_index = 0

            async for text_chunk in self._stream_complete_analysis(
                data.prompt, data.style, data.length
            ):
                if isinstance(text_chunk, dict) and "tokens_used" in text_chunk:
                    total_tokens = text_chunk["tokens_used"]
                    continue

                collected_text += text_chunk
                chunk_data = {
                    "type": "content",
                    "content": text_chunk,
                    "accumulated": collected_text,
                    "timestamp": int(time.time() * 1000),  # ì„œë²„ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
                    "chunk_index": chunk_index,  # ì²­í¬ ìˆœì„œ ë³´ì¥
                }
                chunk_index += 1
                yield json.dumps(chunk_data, ensure_ascii=False)
                # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ê°•ì œ flush
                import asyncio

                await asyncio.sleep(0)  # ì´ë²¤íŠ¸ ë£¨í”„ì— ì œì–´ê¶Œ ì–‘ë³´í•˜ì—¬ ì¦‰ì‹œ ì „ì†¡

            # ì™„ë£Œ í›„ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬ (í‰ë¬¸ í…ìŠ¤íŠ¸)
            generated_text = collected_text.strip()

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë³„ë„ë¡œ ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                # ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ê¸°ë³¸ê°’ ì‚¬ìš©)
                emotion = "í‰ì˜¨"  # ê¸°ë³¸ ê°ì •ìœ¼ë¡œ ì„¤ì •
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë³¸ í”„ë¡¬í”„íŠ¸ì—ì„œ)
                keywords = data.prompt.split()[:5] if data.prompt else []

            except Exception as e:
                logger.warning(f"ìŠ¤íŠ¸ë¦¬ë° í›„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                emotion = "í‰ì˜¨"
                keywords = []

            # ìŠ¤íŠ¸ë¦¬ë° ë¡œê·¸ ì €ì¥
            ai_usage_log = AIUsageLog(
                user_id=user_id,
                api_type="integrated_analysis",
                session_id=session_id,
                request_data=data.model_dump(),
                response_data={
                    "ai_generated_text": generated_text,
                    "emotion": emotion,
                    "keywords": keywords,
                    "style": data.style,
                    "length": data.length,
                },
                tokens_used=total_tokens,
                regeneration_count=regeneration_count,
            )
            self.session.add(ai_usage_log)
            self.session.commit()

            # ì™„ë£Œ ë©”íƒ€ë°ì´í„° ì „ì†¡
            final_data = {
                "type": "complete",
                "emotion": emotion,
                "keywords": keywords,
                "generated_text": generated_text,
                "tokens_used": total_tokens,
                "session_id": session_id,
            }
            yield json.dumps(final_data, ensure_ascii=False)

        except Exception as e:
            logger.error(f"AI í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {str(e)}")
            error_data = {
                "type": "error",
                "error": f"AI í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            }
            yield json.dumps(error_data, ensure_ascii=False)

    async def _stream_complete_analysis(self, prompt: str, style: str, length: str):
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í†µí•© ë¶„ì„ ìˆ˜í–‰"""
        try:
            # ìŠ¤íƒ€ì¼ ë° ê¸¸ì´ ë§¤í•‘
            style_info = {
                "poem": {
                    "name": "ì‹œ",
                    "desc": "ì‹œì ì´ê³  ìš´ìœ¨ì´ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ, ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©",
                },
                "short_story": {
                    "name": "ë‹¨í¸ê¸€",
                    "desc": "ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë¬¸ì²´ë¡œ, ì´ì•¼ê¸°í•˜ë“¯ í¸ì•ˆí•˜ê²Œ",
                },
            }

            length_info = {
                "short": {"name": "ë‹¨ë¬¸", "desc": "1-2ë¬¸ì¥, ìµœëŒ€ 50ì ì´ë‚´"},
                "medium": {"name": "ì¤‘ë¬¸", "desc": "3-5ë¬¸ì¥, ìµœëŒ€ 150ì ì´ë‚´"},
                "long": {"name": "ì¥ë¬¸", "desc": "6-10ë¬¸ì¥, ìµœëŒ€ 300ì ì´ë‚´"},
            }

            style_guide = style_info.get(
                style, {"name": "ë‹¨í¸ê¸€", "desc": "ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë¬¸ì²´ë¡œ"}
            )
            length_guide = length_info.get(length, {"name": "ì¤‘ë¬¸", "desc": "3-5ë¬¸ì¥"})

            system_message = f"""ë‹¹ì‹ ì€ ê°ì„±ì ì´ê³  ìœ„ë¡œê°€ ë˜ëŠ” ê¸€ì„ ì“°ëŠ” ì „ë¬¸ ì‘ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í‚¤ì›Œë“œë‚˜ í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ì„±ì ì¸ ê¸€ê·€ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

- ë¬¸ì²´: {style_guide["name"]} ({style_guide["desc"]})
- ê¸¸ì´: {length_guide["name"]} ({length_guide["desc"]}) - ë°˜ë“œì‹œ ì´ ê¸¸ì´ë¥¼ ì§€ì¼œì£¼ì„¸ìš”
- ë”°ëœ»í•˜ê³  ìœ„ë¡œê°€ ë˜ëŠ” í†¤
- ì¤‘ìš”: ê¸€ê·€ëŠ” ë°˜ë“œì‹œ ìš”ì²­ëœ ê¸¸ì´ ì œí•œ ë‚´ì—ì„œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤

ìƒì„±ëœ ê¸€ê·€ë§Œ ë‹µí•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ JSON í˜•ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"ì‚¬ìš©ì ì…ë ¥: {prompt}"},
            ]

            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ìƒì„± (OpenAI stream=True ì‚¬ìš©)
            import os

            from openai import AsyncOpenAI

            openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

            stream = await openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                max_completion_tokens=500,
                stream=True,
            )

            chunk_count = 0
            total_content = ""

            async for chunk in stream:
                chunk_count += 1
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    total_content += content
                    logger.info(
                        f"ğŸ“¦ OpenAI ì²­í¬ #{chunk_count}: '{content[:50]}...' (ê¸¸ì´: {len(content)})"
                    )

                    # ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì „ì†¡ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼)
                    for char in content:
                        yield char
                        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ê°•ì œ flush
                        import asyncio

                        await asyncio.sleep(0.05)  # 50ms ì§€ì—°ìœ¼ë¡œ ë” í™•ì‹¤í•œ ì‹¤ì‹œê°„ íš¨ê³¼
                else:
                    logger.debug(f"âšª OpenAI ë¹ˆ ì²­í¬ #{chunk_count}")

            logger.info(
                f"ğŸ OpenAI ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: ì´ {chunk_count}ê°œ ì²­í¬, {len(total_content)}ì"
            )

        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise

    def get_regeneration_status(self, session_id: str) -> dict[str, Any]:
        """
        íŠ¹ì • ì„¸ì…˜ì˜ ì¬ìƒì„± íšŸìˆ˜ ì •ë³´ ì¡°íšŒ

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            Dict: ì¬ìƒì„± íšŸìˆ˜ ì •ë³´
        """
        try:
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
            )

            existing_logs = self.session.execute(statement).scalars().all()
            current_count = len(existing_logs)

            return {
                "session_id": session_id,
                "current_count": current_count,
                "max_count": 5,
                "remaining_count": max(0, 5 - current_count),
                "can_regenerate": current_count < 5,
                "is_limit_reached": current_count >= 5,
            }

        except Exception as e:
            logger.error(f"ì¬ìƒì„± ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            raise SessionNotFoundException(session_id=session_id) from e

    async def get_original_user_input(
        self, user_id: UUID, session_id: str
    ) -> str | None:
        """ì„¸ì…˜IDë¡œ ì›ë³¸ ì‚¬ìš©ì ì…ë ¥ ì¡°íšŒ"""
        try:
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.user_id == user_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
                .order_by(AIUsageLog.created_at.asc())
                .limit(1)
            )

            result = self.session.execute(statement).scalar_one_or_none()
            if result and result.request_data:
                import json

                request_data = json.loads(result.request_data)
                return request_data.get("prompt")

            return None

        except Exception as e:
            logger.error(f"ì›ë³¸ ì‚¬ìš©ì ì…ë ¥ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None

    async def regenerate_by_session_id(
        self, user_id: UUID, session_id: str
    ) -> dict[str, Any]:
        """ì„¸ì…˜ IDë¡œ ì´ì „ ìš”ì²­ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ì¬ìƒì„±"""
        try:
            # í•´ë‹¹ ì„¸ì…˜ì˜ ê°€ì¥ ìµœê·¼ ë¡œê·¸ ì¡°íšŒ
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.user_id == user_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
                .order_by(AIUsageLog.created_at.desc())
                .limit(1)
            )

            result = self.session.execute(statement)
            last_log = result.scalar_one_or_none()

            if not last_log:
                raise SessionNotFoundException(session_id=session_id)

            # í˜„ì¬ ì„¸ì…˜ì˜ ì´ ì¬ìƒì„± íšŸìˆ˜ í™•ì¸ (5íšŒ ì œí•œ)
            session_logs_count = self.session.execute(
                select(func.count(AIUsageLog.id))
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
            ).scalar()

            if session_logs_count >= 5:
                raise RegenerationLimitExceededException(
                    current_count=session_logs_count, max_count=5, session_id=session_id
                )

            # ì´ì „ ìš”ì²­ ë°ì´í„° ë³µì›
            original_request = CreateDiaryRequest(**last_log.request_data)

            # ì¬ìƒì„± íšŸìˆ˜ ì¦ê°€
            original_request.regeneration_count = session_logs_count + 1
            original_request.session_id = session_id

            # AI í…ìŠ¤íŠ¸ ì¬ìƒì„±
            return await self.generate_ai_text(user_id, original_request)

        except Exception as e:
            logger.error(f"ì„¸ì…˜ ê¸°ë°˜ ì¬ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise

    def test_db_connection(self) -> dict[str, Any]:
        """
        DB ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸

        Returns:
            Dict: DB ì—°ê²° ìƒíƒœ ì •ë³´
        """
        try:
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ ì‹¤í–‰
            statement = select(func.count()).select_from(AIUsageLog)
            result = self.session.execute(statement).scalar()

            return {
                "status": "success",
                "message": "DB ì—°ê²° ì •ìƒ",
                "total_logs": result,
                "db_session": str(type(self.session)),
                "timestamp": str(datetime.now()),
            }

        except Exception as e:
            logger.error(f"DB ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

            return {
                "status": "error",
                "message": f"DB ì—°ê²° ì‹¤íŒ¨: {str(e)}",
                "error_type": type(e).__name__,
                "timestamp": str(datetime.now()),
            }

    def get_user_daily_stats(self, user_id: UUID) -> dict[str, Any]:
        """
        ì‚¬ìš©ìì˜ ì¼ì¼ AI ì‚¬ìš© í†µê³„ ì¡°íšŒ

        Args:
            user_id: ì‚¬ìš©ì ID

        Returns:
            Dict: ì¼ì¼ AI ì‚¬ìš© í†µê³„
        """
        try:
            from datetime import datetime

            # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì¡°íšŒ
            today = datetime.now(UTC).date()

            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.user_id == user_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
                .where(func.date(AIUsageLog.created_at) == today)
            )

            logs = self.session.execute(statement).scalars().all()

            # ì„¸ì…˜ë³„ í†µê³„
            session_stats = {}
            total_tokens = 0
            total_requests = len(logs)

            for log in logs:
                session_id = str(log.session_id)
                if session_id not in session_stats:
                    session_stats[session_id] = {
                        "session_id": session_id,
                        "request_count": 0,
                        "tokens_used": 0,
                        "first_request": log.created_at,
                        "last_request": log.created_at,
                    }

                session_stats[session_id]["request_count"] += 1
                session_stats[session_id]["tokens_used"] += log.tokens_used or 0
                session_stats[session_id]["last_request"] = max(
                    session_stats[session_id]["last_request"], log.created_at
                )

                total_tokens += log.tokens_used or 0

            return {
                "user_id": user_id,
                "date": today.isoformat(),
                "total_sessions": len(session_stats),
                "total_requests": total_requests,
                "total_tokens_used": total_tokens,
                "average_tokens_per_request": round(total_tokens / total_requests, 2)
                if total_requests > 0
                else 0,
                "sessions": list(session_stats.values()),
            }

        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì¼ì¼ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            raise InvalidRequestException(
                detail=f"ì‚¬ìš©ì í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                field="user_id",
            ) from e

    # validate_request ë©”ì†Œë“œ ì œê±°ë¨ - Pydantic ëª¨ë¸ì—ì„œ ìë™ ê²€ì¦ ì²˜ë¦¬

    async def _generate_complete_analysis(
        self, prompt: str, style: str, length: str
    ) -> dict[str, Any]:
        """
        í•œ ë²ˆì˜ API í˜¸ì¶œë¡œ ê°ì • ë¶„ì„, í‚¤ì›Œë“œ ì¶”ì¶œ, ê¸€ê·€ ìƒì„±ì„ ëª¨ë‘ ì²˜ë¦¬
        """
        try:
            # ìŠ¤íƒ€ì¼ ë° ê¸¸ì´ ë§¤í•‘
            style_info = {
                "poem": {
                    "name": "ì‹œ",
                    "desc": "ì‹œì ì´ê³  ìš´ìœ¨ì´ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ, ì€ìœ ì™€ ìƒì§•ì„ ì‚¬ìš©",
                },
                "short_story": {
                    "name": "ë‹¨í¸ê¸€",
                    "desc": "ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë¬¸ì²´ë¡œ, ì´ì•¼ê¸°í•˜ë“¯ í¸ì•ˆí•˜ê²Œ",
                },
            }

            length_info = {
                "short": {"name": "ë‹¨ë¬¸", "desc": "1-2ë¬¸ì¥, ìµœëŒ€ 50ì ì´ë‚´"},
                "medium": {"name": "ì¤‘ë¬¸", "desc": "3-5ë¬¸ì¥, ìµœëŒ€ 150ì ì´ë‚´"},
                "long": {"name": "ì¥ë¬¸", "desc": "6-10ë¬¸ì¥, ìµœëŒ€ 300ì ì´ë‚´"},
            }

            style_guide = style_info.get(
                style, {"name": "ë‹¨í¸ê¸€", "desc": "ìì—°ìŠ¤ëŸ½ê³  ë”°ëœ»í•œ ë¬¸ì²´ë¡œ"}
            )
            length_guide = length_info.get(length, {"name": "ì¤‘ë¬¸", "desc": "3-5ë¬¸ì¥"})

            system_message = f"""ë‹¹ì‹ ì€ ê°ì„±ì ì´ê³  ìœ„ë¡œê°€ ë˜ëŠ” ê¸€ì„ ì“°ëŠ” ì „ë¬¸ ì‘ê°€ì´ì ì‹¬ë¦¬ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì‘ì—…ì„ í•œ ë²ˆì— ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. ê°ì • ë¶„ì„: ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜
   - í–‰ë³µ, ìŠ¬í””, í™”ë‚¨, í‰ì˜¨, ë¶ˆì•ˆ

2. í‚¤ì›Œë“œ ì¶”ì¶œ: ê°ì • ë‹¨ì–´ë¥¼ ì œì™¸í•œ í•µì‹¬ í‚¤ì›Œë“œ ìµœëŒ€ 5ê°œ (ëª…ì‚¬ ì¤‘ì‹¬)

3. ê¸€ê·€ ìƒì„±:
   - ë¬¸ì²´: {style_guide["name"]} ({style_guide["desc"]})
   - ê¸¸ì´: {length_guide["name"]} ({length_guide["desc"]}) - ë°˜ë“œì‹œ ì´ ê¸¸ì´ë¥¼ ì§€ì¼œì£¼ì„¸ìš”
   - ë”°ëœ»í•˜ê³  ìœ„ë¡œê°€ ë˜ëŠ” í†¤
   - ë¶„ì„ëœ ê°ì •ê³¼ í‚¤ì›Œë“œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë°˜ì˜
   - ì¤‘ìš”: ê¸€ê·€ëŠ” ë°˜ë“œì‹œ ìš”ì²­ëœ ê¸¸ì´ ì œí•œ ë‚´ì—ì„œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "emotion": "ê°ì •",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
    "generated_text": "ìƒì„±ëœ ê¸€ê·€"
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”."""

            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"ì‚¬ìš©ì ì…ë ¥: {prompt}"},
            ]

            client = get_openai_client()
            response = await client.async_chat_completion(
                messages=messages, max_completion_tokens=500
            )
            logger.info(f"OpenAI API ì‘ë‹µ: {response}")

            # JSON íŒŒì‹± (json_repair ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ìœ¼ë¡œ LLM ì‘ë‹µ íŠ¹í™” ì²˜ë¦¬)
            try:
                result_json = response["content"].strip()
                result = None

                try:
                    # json_repairë¥¼ ì‚¬ìš©í•œ ê²¬ê³ í•œ íŒŒì‹±
                    from json_repair import repair_json

                    result = repair_json(result_json, return_objects=True)
                except ImportError:
                    # json_repairê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                    logger.warning(
                        "json_repair ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤"
                    )
                    import re

                    result_json = re.sub(
                        r"[\x00-\x1f\x7f-\x9f\u200b-\u200d\ufeff\u00a0\u2000-\u200a\u2028\u2029]",
                        "",
                        result_json,
                    )
                    result = json.loads(result_json)

                # resultê°€ ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if result is None or not isinstance(result, dict):
                    raise ValueError("íŒŒì‹±ëœ ê²°ê³¼ê°€ ìœ íš¨í•œ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤")

                return {
                    "emotion": result.get("emotion", "í‰ì˜¨"),
                    "keywords": result.get("keywords", [])[:5],
                    "generated_text": result.get("generated_text", ""),
                    "confidence": 0.9,
                    "tokens_used": response["usage"]["total_tokens"],
                }

            except (json.JSONDecodeError, ValueError) as e:
                logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {result_json}, ì˜¤ë¥˜: {str(e)}")
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
                raise AIGenerationFailedException(
                    detail=f"AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
                    error_type="PARSING_ERROR",
                ) from e

        except Exception as e:
            logger.error(f"í†µí•© AI ë¶„ì„ ì‹¤íŒ¨: {str(e)}")

            # OpenAI API ê´€ë ¨ ì˜ˆì™¸ íƒ€ì…ë³„ ì²˜ë¦¬ ë° ì˜ˆì™¸ ë°œìƒ
            error_str = str(e).lower()

            if "rate limit" in error_str or "quota" in error_str:
                # API í˜¸ì¶œ í•œë„ ì´ˆê³¼
                logger.error(f"OpenAI API í˜¸ì¶œ í•œë„ ì´ˆê³¼: {str(e)}")
                raise AIGenerationFailedException(
                    detail="OpenAI API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    error_type="RATE_LIMIT_ERROR",
                ) from e
            elif "service unavailable" in error_str or "timeout" in error_str:
                # ì„œë¹„ìŠ¤ ì¼ì‹œì  ë¶ˆê°€
                logger.error(f"OpenAI ì„œë¹„ìŠ¤ ì¼ì‹œì  ë¶ˆê°€: {str(e)}")
                raise AIGenerationFailedException(
                    detail="AI ì„œë¹„ìŠ¤ê°€ ì¼ì‹œì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    error_type="SERVICE_UNAVAILABLE",
                ) from e
            elif "token" in error_str and "limit" in error_str:
                # í† í° í•œë„ ì´ˆê³¼
                logger.error(f"í† í° í•œë„ ì´ˆê³¼: {str(e)}")
                raise AIGenerationFailedException(
                    detail="ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë” ì§§ì€ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    error_type="TOKEN_LIMIT_ERROR",
                ) from e
            else:
                # ê¸°íƒ€ AI ìƒì„± ì˜¤ë¥˜
                logger.error(f"AI ìƒì„± ì˜¤ë¥˜: {str(e)}")
                raise AIGenerationFailedException(
                    detail=f"AI í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                    error_type="GENERATION_ERROR",
                ) from e

    async def stream_regenerate_by_session_id(self, user_id: UUID, session_id: str):
        """ì„¸ì…˜ IDë¡œ ì´ì „ ìš”ì²­ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ ìŠ¤íŠ¸ë¦¬ë° ì¬ìƒì„±"""
        try:
            logger.info(
                f"ì¬ìƒì„± ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: session_id={session_id}, user_id={user_id}"
            )

            # í•´ë‹¹ ì„¸ì…˜ì˜ ê°€ì¥ ìµœê·¼ ë¡œê·¸ ì¡°íšŒ
            statement = (
                select(AIUsageLog)
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.user_id == user_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
                .order_by(AIUsageLog.created_at.desc())
                .limit(1)
            )

            result = self.session.execute(statement)
            last_log = result.scalar_one_or_none()

            if not last_log:
                error_data = {
                    "type": "error",
                    "error": f"ì„¸ì…˜ ID {session_id}ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                }
                yield json.dumps(error_data, ensure_ascii=False)
                return

            # í˜„ì¬ ì„¸ì…˜ì˜ ì´ ì¬ìƒì„± íšŸìˆ˜ í™•ì¸ (5íšŒ ì œí•œ)
            session_logs_count = self.session.execute(
                select(func.count(AIUsageLog.id))
                .where(AIUsageLog.session_id == session_id)
                .where(AIUsageLog.api_type == "integrated_analysis")
            ).scalar()

            if session_logs_count >= 5:
                error_data = {
                    "type": "error",
                    "error": "ì¬ìƒì„± íšŸìˆ˜ê°€ 5íšŒë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.",
                    "session_id": session_id,
                    "current_count": session_logs_count,
                }
                yield json.dumps(error_data, ensure_ascii=False)
                return

            # ì´ì „ ìš”ì²­ ë°ì´í„° ë³µì›
            import json as json_lib

            original_request_data = (
                json_lib.loads(last_log.request_data)
                if isinstance(last_log.request_data, str)
                else last_log.request_data
            )
            original_request = CreateDiaryRequest(**original_request_data)

            # ìƒˆë¡œìš´ ì¬ìƒì„± íšŸìˆ˜ë¡œ ì„¤ì •
            new_regeneration_count = session_logs_count + 1

            # ì´ˆê¸° ë©”íƒ€ë°ì´í„° ì „ì†¡
            initial_data = {
                "type": "start",
                "session_id": session_id,
                "regeneration_count": new_regeneration_count,
            }
            yield json.dumps(initial_data, ensure_ascii=False)

            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ stream_ai_textì™€ ë™ì¼í•œ ë¡œì§)
            collected_text = ""
            total_tokens = 0
            chunk_index = 0

            async for text_chunk in self._stream_complete_analysis(
                original_request.prompt, original_request.style, original_request.length
            ):
                if isinstance(text_chunk, dict) and "tokens_used" in text_chunk:
                    total_tokens = text_chunk["tokens_used"]
                    continue

                collected_text += text_chunk
                chunk_data = {
                    "type": "content",
                    "content": text_chunk,
                    "accumulated": collected_text,
                    "timestamp": int(time.time() * 1000),
                    "chunk_index": chunk_index,
                }
                chunk_index += 1
                yield json.dumps(chunk_data, ensure_ascii=False)

            # ì™„ë£Œ í›„ ë¶„ì„ ê²°ê³¼ ì²˜ë¦¬
            generated_text = collected_text.strip()

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ë³„ë„ë¡œ ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                # ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ê¸°ë³¸ê°’ ì‚¬ìš©)
                emotion = "í‰ì˜¨"
                # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë³¸ í”„ë¡¬í”„íŠ¸ì—ì„œ)
                keywords = (
                    original_request.prompt.split()[:5]
                    if original_request.prompt
                    else []
                )
            except Exception as e:
                logger.warning(f"ì¬ìƒì„± ìŠ¤íŠ¸ë¦¬ë° í›„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                emotion = "í‰ì˜¨"
                keywords = []

            # ì¬ìƒì„± ìŠ¤íŠ¸ë¦¬ë° ë¡œê·¸ ì €ì¥
            ai_usage_log = AIUsageLog(
                user_id=user_id,
                api_type="integrated_analysis",
                session_id=session_id,
                request_data=original_request_data,
                response_data={
                    "ai_generated_text": generated_text,
                    "emotion": emotion,
                    "keywords": keywords,
                    "style": original_request.style,
                    "length": original_request.length,
                },
                tokens_used=total_tokens,
                regeneration_count=new_regeneration_count,
            )
            self.session.add(ai_usage_log)
            self.session.commit()

            # ì™„ë£Œ ë©”íƒ€ë°ì´í„° ì „ì†¡
            final_data = {
                "type": "complete",
                "emotion": emotion,
                "keywords": keywords,
                "generated_text": generated_text,
                "tokens_used": total_tokens,
                "session_id": session_id,
                "regeneration_count": new_regeneration_count,
            }
            yield json.dumps(final_data, ensure_ascii=False)

            logger.info(
                f"ì¬ìƒì„± ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: session_id={session_id}, tokens={total_tokens}"
            )

        except Exception as e:
            logger.error(
                f"ì¬ìƒì„± ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: session_id={session_id}, error={str(e)}"
            )
            error_data = {
                "type": "error",
                "error": f"ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            }
            yield json.dumps(error_data, ensure_ascii=False)
